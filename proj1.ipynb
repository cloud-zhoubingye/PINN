{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDE:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\frac{\\partial^2 u}{\\partial x^2}-\\frac{\\partial^4 u}{\\partial y^4}=\\left(2-x^2\\right) e^{-y}\\\\\n",
    "&\\begin{aligned} \n",
    "\\end{aligned}\n",
    "\\end{aligned}\n",
    "$$\n",
    "IC/BC:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\begin{aligned} \n",
    "u_{y y}(x, 0) & =x^2 \\\\\n",
    "u_{y y}(x, 1) & =\\frac{x^2}{e} \\\\\n",
    "u(x, 0) & =x^2 \\\\\n",
    "u(x, 1) & =\\frac{x^2}{e} \\\\\n",
    "u(0, y) & =0 \\\\\n",
    "u(1, y) & =e^{-y}\n",
    "\\end{aligned}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Analytical Solution:\n",
    "$$\n",
    "y=x^{2} \\cdot e^{-y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "epoches=1500\n",
    "h_test=1000\n",
    "N_inner_point=1000\n",
    "N_boundary_point=100\n",
    "N_pde_point=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置随机种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_seed(seed):\n",
    "    torch.manual_seed(seed=seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed=seed)\n",
    "initial_seed(8080);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 内点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner(n=N_inner_point):\n",
    "    x=torch.rand(n,1).requires_grad_(True)   #生成n*1维的0-1均匀分布随机点\n",
    "    y=torch.rand(n,1).requires_grad_(True)\n",
    "    cond=(2-x**2)*torch.exp(-y)\n",
    "    return x,y,cond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 内点真实解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_analytical_solution(n=N_pde_point):\n",
    "    x=torch.rand(n,1).requires_grad_(True)\n",
    "    y=torch.rand(n,1).requires_grad_(True)\n",
    "    cond=(x**2)*torch.exp(-y)\n",
    "    return x,y,cond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 边界点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def u_yy_down(n=N_boundary_point):\n",
    "     x=torch.rand(n,1).requires_grad_(True)\n",
    "     y=torch.zeros_like(x).requires_grad_(True)\n",
    "     cond=x**2\n",
    "     return x,y,cond\n",
    "\n",
    "def u_yy_upper(n=N_boundary_point):\n",
    "    x=torch.rand(n,1).requires_grad_(True)\n",
    "    y=torch.ones_like(x).requires_grad_(True)\n",
    "    cond=x**2/torch.e\n",
    "    return x,y,cond\n",
    "\n",
    "def u_down(n=N_boundary_point):\n",
    "     x=torch.rand(n,1).requires_grad_(True)\n",
    "     y=torch.zeros_like(x).requires_grad_(True)\n",
    "     cond=x**2\n",
    "     return x,y,cond\n",
    "\n",
    "def u_upper(n=N_boundary_point):\n",
    "    x=torch.rand(n,1).requires_grad_(True)\n",
    "    y=torch.ones_like(x).requires_grad_(True)\n",
    "    cond=x**2/torch.e\n",
    "    return x,y,cond\n",
    "\n",
    "def u_left(n=N_boundary_point):\n",
    "     y=torch.rand(n,1).requires_grad_(True)\n",
    "     x=torch.zeros_like(y).requires_grad_(True)\n",
    "     cond=torch.zeros_like(x).requires_grad_(True)\n",
    "     return x,y,cond\n",
    "\n",
    "def u_right(n=N_boundary_point):\n",
    "     y=torch.rand(n,1).requires_grad_(True)\n",
    "     x=torch.ones_like(y).requires_grad_(True)\n",
    "     cond=torch.exp(-y)\n",
    "     return x,y,cond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP全连接神经网络\n",
    "##### 2维输入，64维隐藏层，1维输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP,self).__init__()\n",
    "        self.net=torch.nn.Sequential(\n",
    "            torch.nn.Linear(2,32),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(32,64),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(64,64),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(64,64),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(64,32),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(32,1)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE损失、高阶求导\n",
    "#### ???[grad\\_outputs](https://blog.csdn.net/waitingwinter/article/details/105774720)???\n",
    "先假设 $x, y$ 为一维向量, 即可设自变量因变量分别为 $\\mathbf{x}=\\left(x_1, x_2, \\cdots, x_n\\right) \\in \\mathbb{R}^n, y=f(\\mathbf{x})=\\left(y_1, y_2, \\cdots, y_m\\right) \\in \\mathbb{R}^m$ ，其对应的 Jacobian 矩阵为\n",
    "$$\n",
    "J=\\left(\\begin{array}{cccc}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} & \\cdots & \\frac{\\partial y_1}{\\partial x_n} \\\\\n",
    "\\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} & \\cdots & \\frac{\\partial y_2}{\\partial x_n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_m}{\\partial x_1} & \\frac{\\partial y_m}{\\partial x_2} & \\cdots & \\frac{\\partial y_m}{\\partial x_n}\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "grad_outputs 是一个shape 与 outputs 一致的向量, 即\n",
    "$$\n",
    "{grad\\_outputs}=(\\begin{array}{llll}\n",
    "a_{11} & a_{12} & \\cdots & a_{1 m}\n",
    "\\end{array})^T,\n",
    "$$\n",
    "在给定grad_outputs 之后，真正返回的梯度为\n",
    "$$\n",
    "grad=\\left(\\begin{array}{c}\n",
    "a_1 \\frac{\\partial y_1}{\\partial x_1}+a_2 \\frac{\\partial y_2}{\\partial x_1}+\\cdots+a_m \\frac{\\partial y_m}{\\partial x_1} \\\\\n",
    "a_1 \\frac{\\partial y_1}{\\partial x_2}+a_2 \\frac{\\partial y_2}{\\partial x_2}+\\cdots+a_m \\frac{\\partial y_m}{\\partial x_2} \\\\\n",
    "\\cdots \\cdots \\cdots \\cdots \\\\\n",
    "a_1 \\frac{\\partial y_1}{\\partial x_n} + a_2 \\frac{\\partial y_2}{\\partial x_n} + \\cdots + a_m \\frac{\\partial y_m}{\\partial x_n}\n",
    "\\end{array}\\right) \\in \\mathbb{R}^n\n",
    "$$\n",
    "为方便下文叙述我们引入记号 grad $=J \\otimes g r a d \\_o u t p u t s$.\n",
    "其次假设 $x=\\left(x_1, \\cdots, x_n\\right) \\in \\mathbb{R}^n, \\mathbf{y}=\\left(\\mathbf{y}_1, \\cdots, \\mathbf{y}_{\\mathbf{t}}\\right) \\in \\mathbb{R}^{s \\times t}$, 第 $\\mathrm{i}$ 个列向量对应的Jacobi矩阵为\n",
    "$$\n",
    "J_i, 1 \\leq i \\leq t\n",
    "$$\n",
    "此时的grad_outputs 为(维度与outputs一致)\n",
    "$$\n",
    "{ grad\\_outputs }=\\left(g o_1, \\cdots, g o_t\\right) \\in \\mathbb{R}^{s \\times t}\n",
    "$$\n",
    "由第一种情况，我们有\n",
    "$$\n",
    "grad=\\sum_{i=1}^t J_i \\otimes g o_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#均方误差损失函数\n",
    "loss=torch.nn.MSELoss()\n",
    "\n",
    "#高阶递归求导\n",
    "def gradients(u,x,derivative_order=1):\n",
    "    if derivative_order==1:\n",
    "        #grad_outputs:待求导函数为矢量时需要添加的一个shape与outputs一致的向量\n",
    "        #create_graph:若要计算高阶导数，则必须选为True\n",
    "        return torch.autograd.grad(u,x,grad_outputs=torch.ones_like(u),create_graph=True,only_inputs=True)[0]\n",
    "    else:\n",
    "        #求u(x,y)的order次导数 == 求u'(x,y)的(order-1)次导数\n",
    "        return gradients(gradients(u,x),x,derivative_order=derivative_order-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 内点损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_inner(u):\n",
    "    x,y,cond=inner()\n",
    "    u_pred=u(torch.cat([x,y],dim=1))\n",
    "    return loss(gradients(u_pred,x,2)-gradients(u_pred,y,4),cond)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_data(u):\n",
    "    x,y,cond=inner_analytical_solution()\n",
    "    u_pred=u(torch.cat([x,y],dim=1))\n",
    "    return loss(u_pred,cond)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 边界点损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_yy_down(u):\n",
    "     x,y,cond=u_yy_down()\n",
    "     u_pred=u(torch.cat([x,y],dim=1))\n",
    "     return loss(gradients(u_pred,y,2),cond)\n",
    "\n",
    "def loss_yy_upper(u):\n",
    "     x,y,cond=u_yy_upper()\n",
    "     u_pred=u(torch.cat([x,y],dim=1))\n",
    "     return loss(gradients(u_pred,y,2),cond)\n",
    "\n",
    "def loss_down(u):\n",
    "     x,y,cond=u_down()\n",
    "     u_pred=u(torch.cat([x,y],dim=1))\n",
    "     return loss(u_pred,cond)\n",
    "\n",
    "def loss_upper(u):\n",
    "     x,y,cond=u_upper()\n",
    "     u_pred=u(torch.cat([x,y],dim=1))\n",
    "     return loss(u_pred,cond)\n",
    "\n",
    "def loss_left(u):\n",
    "     x,y,cond=u_left()\n",
    "     u_pred=u(torch.cat([x,y],dim=1))\n",
    "     return loss(u_pred,cond)\n",
    "\n",
    "def loss_right(u):\n",
    "     x,y,cond=u_right()\n",
    "     u_pred=u(torch.cat([x,y],dim=1))\n",
    "     return loss(u_pred,cond)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning Epoches:  0  Training Loss:  tensor(2.1144, grad_fn=<MeanBackward0>)\n",
      "Traning Epoches:  100  Training Loss:  tensor(0.2069, grad_fn=<MeanBackward0>)\n",
      "Traning Epoches:  200  Training Loss:  tensor(0.0518, grad_fn=<MeanBackward0>)\n",
      "Traning Epoches:  300  Training Loss:  tensor(0.0232, grad_fn=<MeanBackward0>)\n",
      "Traning Epoches:  400  Training Loss:  tensor(0.0127, grad_fn=<MeanBackward0>)\n",
      "Traning Epoches:  500  Training Loss:  tensor(0.0054, grad_fn=<MeanBackward0>)\n",
      "Traning Epoches:  600  Training Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
      "Traning Epoches:  700  Training Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
      "Traning Epoches:  800  Training Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
      "Traning Epoches:  900  Training Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
      "Traning Epoches:  1000  Training Loss:  tensor(0.0002, grad_fn=<MeanBackward0>)\n",
      "Traning Epoches:  1100  Training Loss:  tensor(0.0002, grad_fn=<MeanBackward0>)\n",
      "Traning Epoches:  1200  Training Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
      "Traning Epoches:  1300  Training Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
      "Traning Epoches:  1400  Training Loss:  tensor(0.0002, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "u=MLP()\n",
    "#使用Adam优化器，学习率自适应\n",
    "optimer=torch.optim.Adam(params=u.parameters())\n",
    "\n",
    "for i in range(epoches):\n",
    "    optimer.zero_grad()\n",
    "    training_loss=loss_inner(u)+loss_yy_down(u)+loss_yy_upper(u)+loss_down(u)+loss_upper(u)+loss_left(u)+loss_right(u)+loss_data(u)\n",
    "    training_loss.backward()\n",
    "    optimer.step()\n",
    "    #打印进度\n",
    "    if i%100==0:\n",
    "        print(\"Traning Epoches: \",i,\" Training Loss: \",torch.mean(training_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测并检验效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max error is  0.024331748485565186\n",
      "Average error is  0.016554679721593857\n",
      "Pred: \n",
      " tensor([[-0.0161],\n",
      "        [-0.0161],\n",
      "        [-0.0161],\n",
      "        ...,\n",
      "        [ 0.3503],\n",
      "        [ 0.3500],\n",
      "        [ 0.3496]], grad_fn=<AddmmBackward0>)\n",
      "Real: \n",
      " tensor([[0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        ...,\n",
      "        [0.3686],\n",
      "        [0.3682],\n",
      "        [0.3679]])\n",
      "Error: \n",
      " tensor([[0.0161],\n",
      "        [0.0161],\n",
      "        [0.0161],\n",
      "        ...,\n",
      "        [0.0183],\n",
      "        [0.0183],\n",
      "        [0.0182]], grad_fn=<AbsBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#生成0-1均匀分布的点\n",
    "x_uniform=torch.linspace(0,1,h_test)\n",
    "#生成二维网格\n",
    "xx,yy=torch.meshgrid(x_uniform,x_uniform)\n",
    "xx,yy=xx.reshape(-1,1),yy.reshape(-1,1)\n",
    "#将1*n的横坐标集(x1,x2,...,xn)和纵坐标集(y1,y2,...,yn)\n",
    "#先变为{(x1),(x2),...,(xn)}和{(y1),(y2),...,(yn)}，即n*1\n",
    "#再变为网格点{(x1,y1),(x2,y2),...,(xn,yn)}的序列，即n*2\n",
    "xy=torch.cat([xx,yy],dim=1)\n",
    "xy_pred=u(xy)\n",
    "\n",
    "#误差\n",
    "xy_real=xx**2*torch.exp(-yy)\n",
    "error=torch.abs(xy_pred-xy_real)\n",
    "print(\"Max error is \",float(torch.max(error)))\n",
    "print(\"Average error is \",float(torch.mean(error)))\n",
    "print(\"Pred: \\n\",xy_pred)\n",
    "print(\"Real: \\n\",xy_real)\n",
    "print(\"Error: \\n\",error)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
